{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_typo_per_ego():\n",
    "    main_dir = 'facebook_k5'\n",
    "    typo_per_ego_per_k = {}\n",
    "    for classe in range(2,16):\n",
    "        for temp_classe in range(classe):\n",
    "            with open('../Results/%s/Typo_%s/egos_classe_%s.csv' % (main_dir, classe, temp_classe), 'r') as to_read:\n",
    "                csv_r = csv.reader(to_read, delimiter = ';')\n",
    "                en_tete = csv_r.next()\n",
    "                for line in csv_r:\n",
    "                    ego = line[0]\n",
    "                    if not ego in typo_per_ego_per_k:\n",
    "                        typo_per_ego_per_k[ego] = {}\n",
    "                    typo_per_ego_per_k[ego][int(classe)] = int(temp_classe)\n",
    "    \n",
    "    with open('../Results/%s/typo_par_ego.csv' % main_dir, 'w') as to_write:\n",
    "        csv_w = csv.writer(to_write, delimiter = ';')\n",
    "        csv_w.writerow(['ego'] + ['k = %s' % temp_classe for temp_classe in range(2,16)])\n",
    "        for ego in typo_per_ego_per_k:\n",
    "            csv_w.writerow([ego] + [typo_per_ego_per_k[ego][classe] for classe in range(2,16)])\n",
    "\n",
    "def create_all_rel_sankey():\n",
    "    \n",
    "    suivants_par_cluster = {}\n",
    "    size_par_cluster = {}\n",
    "    \n",
    "    with open(MAIN_DIR+'/typo_par_ego.csv', 'r') as to_read:\n",
    "        csv_r = csv.reader(to_read, delimiter = ';')\n",
    "        \n",
    "        first_line = csv_r.next()\n",
    "        nb_col = len(first_line) - 1\n",
    "        \n",
    "        for line in csv_r:\n",
    "            id_premier_groupe = 2\n",
    "            for elem in line[id_premier_groupe-1:]:\n",
    "                if not '%s_%s' % (id_premier_groupe, elem) in suivants_par_cluster:\n",
    "                    size_par_cluster['%s_%s' % (id_premier_groupe, elem)] = 1\n",
    "                    suivants_par_cluster['%s_%s' % (id_premier_groupe, elem)] = {}\n",
    "                else:\n",
    "                    size_par_cluster['%s_%s' % (id_premier_groupe, elem)] += 1\n",
    "                id_second_groupe = id_premier_groupe + 1\n",
    "                suivants_groupe_1 = suivants_par_cluster['%s_%s' % (id_premier_groupe, elem)]\n",
    "                for elem_2 in line[id_second_groupe-1:]:\n",
    "                    if not '%s_%s' % (id_second_groupe, elem_2) in suivants_groupe_1:\n",
    "                        suivants_groupe_1['%s_%s' % (id_second_groupe, elem_2)] = 1\n",
    "                    else:\n",
    "                        suivants_groupe_1['%s_%s' % (id_second_groupe, elem_2)] += 1\n",
    "                    id_second_groupe += 1\n",
    "                id_premier_groupe += 1\n",
    "                \n",
    "    liste_clusters_target = set()\n",
    "    \n",
    "    for cluster_origine in suivants_par_cluster:\n",
    "        keys = suivants_par_cluster[cluster_origine]\n",
    "        for key in keys:\n",
    "            liste_clusters_target.add(key)\n",
    "            \n",
    "    liste_clusters_target = list(liste_clusters_target)\n",
    "    liste_clusters_target.sort(key = lambda x : (int(x.split('_')[0]), int(x.split('_')[1])))\n",
    "        \n",
    "    liste_clusters_source = [x for x in suivants_par_cluster.keys() if not suivants_par_cluster[x] == {}]\n",
    "    liste_clusters_source.sort()\n",
    "    \n",
    "            \n",
    "    with open(MAIN_DIR+'/all_rel_sankey.csv', 'w') as to_write:\n",
    "        csv_w = csv.writer(to_write, delimiter = ';')\n",
    "        first_line = []\n",
    "        k_former_target = 2\n",
    "        for cluster_target in liste_clusters_target:\n",
    "            k_this_target = int(cluster_target.split('_')[0])\n",
    "            if k_this_target > k_former_target:\n",
    "                first_line.append('')\n",
    "            first_line.append(cluster_target.split('_')[0]+'_'+str(int(cluster_target.split('_')[1])+1))\n",
    "            k_former_target = k_this_target\n",
    "        \n",
    "        csv_w.writerow(first_line)\n",
    "        \n",
    "        k_former_source = 2\n",
    "        \n",
    "        for source in liste_clusters_source:            \n",
    "            line_to_write = [source.split('_')[0]+'_'+str(int(source.split('_')[1])+1)]\n",
    "            k_this_source = int(source.split('_')[0])\n",
    "            if k_this_source > k_former_source:\n",
    "                csv_w.writerow([])\n",
    "            k_former_target = 3\n",
    "            \n",
    "            for target in liste_clusters_target:\n",
    "                k_this_target = int(target.split('_')[0])\n",
    "                if k_this_target > k_former_target:\n",
    "                    line_to_write.append('')           \n",
    "                if k_this_target <= k_this_source:\n",
    "                    line_to_write.append('')\n",
    "                    k_former_target = k_this_target\n",
    "                    continue\n",
    "                k_former_target = k_this_target\n",
    "                percent = round(100*suivants_par_cluster[source].get(target, 0) / float(size_par_cluster[source]), 1)\n",
    "                line_to_write.append(percent)\n",
    "            \n",
    "            csv_w.writerow(line_to_write)\n",
    "            k_former_source = k_this_source\n",
    "            \n",
    "    with open('sankey.html', 'r') as to_read:\n",
    "        with open(MAIN_DIR+'/sankey.html', 'w') as to_write:\n",
    "            for line in to_read:\n",
    "                to_write.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_all_sankey():\n",
    "    sources_per_target, targets_per_source, size_per_target, size_per_source = read_typos()\n",
    "    list_sources = targets_per_source.keys()\n",
    "    list_sources.sort(key = lambda x : (int(x.split('_')[0]), int(x.split('_')[1])))\n",
    "    with open(MAIN_DIR+'/all_rel_sankey.csv', 'w') as to_write:\n",
    "        csv_w = csv.writer(to_write, delimiter = ',')\n",
    "        csv_w.writerow(['source','target','value'])\n",
    "\n",
    "        for source in list_sources:            \n",
    "            for target in targets_per_source[source]:       \n",
    "                csv_w.writerow([source.split('_')[0]+'_'+str(int(source.split('_')[1])),\n",
    "                                target.split('_')[0]+'_'+str(int(target.split('_')[1])),\n",
    "                                sources_per_target[target][source]])\n",
    "\n",
    "    with open('sankey.html', 'r') as to_read:\n",
    "        with open(MAIN_DIR+'/sankey.html', 'w') as to_write:\n",
    "            for line in to_read:\n",
    "                to_write.write(line) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_typos():\n",
    "    sources_per_target = {}\n",
    "    size_per_target = {}\n",
    "    targets_per_source = {}\n",
    "    size_per_source = {}\n",
    "    \n",
    "    with open(MAIN_DIR+'/typo_par_ego.csv', 'r') as to_read:\n",
    "        csv_r = csv.reader(to_read, delimiter = ';')\n",
    "        csv_r.next()\n",
    "        \n",
    "        for line in csv_r:\n",
    "            for k_source in range(1,len(line)-1):                \n",
    "                source = '%s_%s' % (k_source+1, line[k_source])\n",
    "                target = '%s_%s' % (k_source+2, line[k_source+1])\n",
    "                \n",
    "                sources_per_target[target] = sources_per_target.get(target, {})\n",
    "                targets_per_source[source] = targets_per_source.get(source, {})\n",
    "                size_per_target[target] = size_per_target.get(target, 0) + 1\n",
    "                size_per_source[source] = size_per_source.get(source, 0) + 1\n",
    "                \n",
    "                sources_per_target[target][source] = sources_per_target[target].get(source, 0) + 1\n",
    "                targets_per_source[source][target] = targets_per_source[source].get(target, 0) + 1\n",
    "    \n",
    "    return sources_per_target, targets_per_source, size_per_target, size_per_source\n",
    " \n",
    "    \n",
    "def create_sankey():\n",
    "\n",
    "    sources_per_target, targets_per_source, size_per_target, size_per_source = read_typos()\n",
    "    list_sources = targets_per_source.keys()\n",
    "    list_sources.sort(key = lambda x : (int(x.split('_')[0]), int(x.split('_')[1])))\n",
    "    with open(MAIN_DIR+'/sankey.csv', 'w') as to_write:\n",
    "        csv_w = csv.writer(to_write, delimiter = ',')\n",
    "        csv_w.writerow(['source','target','value'])\n",
    "        \n",
    "        for source in list_sources:            \n",
    "            for target in targets_per_source[source]:       \n",
    "                csv_w.writerow([source.split('_')[0]+'_'+str(int(source.split('_')[1])),\n",
    "                                target.split('_')[0]+'_'+str(int(target.split('_')[1])),\n",
    "                                sources_per_target[target][source]])\n",
    "                \n",
    "    with open('sankey.html', 'r') as to_read:\n",
    "        with open(MAIN_DIR+'/sankey.html', 'w') as to_write:\n",
    "            for line in to_read:\n",
    "                to_write.write(line)\n",
    "                \n",
    "\n",
    "def percent_sources_targets(sources_per_target, targets_per_source, size_per_target, size_per_source):\n",
    "    list_targets = sources_per_target.keys()\n",
    "    list_targets.sort(key = lambda x : (int(x.split('_')[0]), int(x.split('_')[1])), reverse = True)\n",
    "    \n",
    "    percent_sources_per_target = {}\n",
    "    for target in list_targets:\n",
    "        percent_sources_per_target[target] = {}\n",
    "        for source in sources_per_target[target]:\n",
    "            percent_sources_per_target[target][source] = 100*sources_per_target[target][source] / float(size_per_target[target])\n",
    "    \n",
    "    list_sources = targets_per_source.keys()\n",
    "    list_sources.sort(key = lambda x : (int(x.split('_')[0]), int(x.split('_')[1])))\n",
    "    percent_targets_per_source = {}\n",
    "    \n",
    "    for source in list_sources:\n",
    "        percent_targets_per_source[source] = {}\n",
    "        for target in targets_per_source[source]:\n",
    "            percent_targets_per_source[source][target] = 100*targets_per_source[source][target] / float(size_per_source[source])\n",
    "\n",
    "    return list_sources, targets_per_source, percent_sources_per_target, percent_targets_per_source\n",
    "            \n",
    "def create_simple_sankey():\n",
    "    sources_per_target, targets_per_source, size_per_target, size_per_source = read_typos()\n",
    "    \n",
    "    list_sources, targets_per_source, \\\n",
    "    percent_sources_per_target, percent_targets_per_source = \\\n",
    "        percent_sources_targets(sources_per_target, targets_per_source, size_per_target, size_per_source)\n",
    "    sources_ok = ['2_0', '2_1']\n",
    "    was_already_coherent = []\n",
    "    coherence_before_coherence = {}\n",
    "    \n",
    "    with open(MAIN_DIR+'/simple_sankey.csv', 'w') as to_write:\n",
    "        csv_w = csv.writer(to_write, delimiter = ',')\n",
    "        csv_w.writerow(['source','target','value'])\n",
    "        \n",
    "        for source in list_sources:\n",
    "            #Si la source n'est pas dispo\n",
    "            #on ne la calcule pas\n",
    "            if not source in sources_ok: \n",
    "                continue\n",
    "                \n",
    "            link_exists, link_is_all = False, False    \n",
    "            for target in targets_per_source[source]:\n",
    "                lvl = int(target.split('_')[0])-2\n",
    "                \n",
    "                v_same = max(65, VALUE_SAME * (1 - lvl/10.))\n",
    "                v_min = VALUE_MIN_EXISTS\n",
    "                \n",
    "                #Si il y a moins de x% de passage de l'un à l'autre, on la trouvera autrement \n",
    "                #ou pas\n",
    "                if percent_targets_per_source[source][target] < VALUE_MIN_EXISTS:\n",
    "                    continue\n",
    "                    \n",
    "                #Si ça ne suffit pas pour s'arrêter on le note dans les sources potentielles\n",
    "                if (percent_sources_per_target[target][source] > v_same \\\n",
    "                    and percent_targets_per_source[source][target] > v_same):\n",
    "                        gchild_ok = False\n",
    "                        if not target in percent_targets_per_source:\n",
    "                            gchild_ok = True\n",
    "                            continue\n",
    "                        for gtarget in percent_targets_per_source[target]:\n",
    "                            if (percent_targets_per_source[target][gtarget] > v_same \\\n",
    "                                and percent_targets_per_source[source][target] > v_same):\n",
    "                                gchild_ok = True\n",
    "                        if gchild_ok:\n",
    "                            continue\n",
    "                \n",
    "                sources_ok.append(target)   \n",
    "                csv_w.writerow([source.split('_')[0]+'_'+str(int(source.split('_')[1])),\n",
    "                            target.split('_')[0]+'_'+str(int(target.split('_')[1])),\n",
    "                            sources_per_target[target][source]])\n",
    "\n",
    "    with open('simple_sankey.html', 'r') as to_read:\n",
    "        with open(MAIN_DIR+'/simple_sankey.html', 'w') as to_write:\n",
    "            for line in to_read:\n",
    "                to_write.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.0, 90.0, 80.0, 70.0, 60, 60, 60, 60, 60, 60]\n"
     ]
    }
   ],
   "source": [
    "VALUE_SAME = 100.0\n",
    "VALUE_MIN_EXISTS = 30.0\n",
    "\n",
    "print [max(60, VALUE_SAME * (1 - lvl/10.)) for lvl in range (10)]\n",
    "\n",
    "create_simple_sankey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = 'facebook'\n",
    "k = 5\n",
    "\n",
    "FILE_NAME = '%s_k%s' % (corpus, k)\n",
    "MAIN_DIR = '../Results/%s' % FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_typo_per_ego()\n",
    "create_all_rel_sankey()\n",
    "create_sankey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dir(k_value):\n",
    "    directory = '%s/Typo_%s' % (MAIN_DIR, k_value)\n",
    "    if not 'Typo_%s' % k_value in os.listdir(MAIN_DIR):\n",
    "        os.mkdir(directory)\n",
    "    return directory\n",
    "\n",
    "def get_data(file_name = None, representativity = True):\n",
    "    if not file_name:\n",
    "        file_in = '../Data/%s.csv' % FILE_NAME\n",
    "    else:\n",
    "        file_in = file_name\n",
    "    egos, typo_vector = [], {}\n",
    "    nb_par_ego = {}\n",
    "    with open(file_in, 'r') as to_read:\n",
    "        csv_r = csv.reader(to_read, delimiter = ';')\n",
    "        \n",
    "        first_line = csv_r.next()\n",
    "        keys_typo = [int(x.split('_')[-1]) for x in first_line[1:len(first_line)]]\n",
    "        \n",
    "        for line in csv_r:\n",
    "            ego = line[0]\n",
    "            if ego in egos:\n",
    "                ego = ego + '_%s' % (nb_par_ego.get(ego, 0) + 1)\n",
    "                nb_par_ego[ego] = 1 + nb_par_ego.get(ego, 0)\n",
    "            egos.append(ego)\n",
    "            if not representativity:\n",
    "                typo_vector[ego] = {int(keys_typo[i]) : float(line[i+1]) for i in range(len(keys_typo))}\n",
    "            else:\n",
    "                typo_vector[ego] = \\\n",
    "                {\n",
    "                    int(keys_typo[i]) : float(line[i+1]) if float(line[i+1]) <= 1 \\\n",
    "                                        else 2-1/float(line[i+1])\n",
    "\n",
    "                    for i in range(len(keys_typo))\n",
    "                }\n",
    "                    \n",
    "                \n",
    "    return egos, typo_vector, keys_typo\n",
    "\n",
    "def get_overall_representativity():\n",
    "    s = {x : 0 for x in DATA_MOTIFS[DATA_MOTIFS.keys()[0]]}     \n",
    "       \n",
    "    for ego in DATA_MOTIFS:    \n",
    "        for graphlet in DATA_MOTIFS[ego]:\n",
    "            s[graphlet] += int(DATA_MOTIFS[ego][graphlet])\n",
    "    \n",
    "    all_graphlets = sum(s.values())\n",
    "    return {x : s[x]/float(all_graphlets) for x in s}\n",
    "\n",
    "\n",
    "def get_classe_representativity(list_of_egos):\n",
    "    s = {x : 0 for x in DATA_MOTIFS[DATA_MOTIFS.keys()[0]]}\n",
    "    for ego in list_of_egos:\n",
    "        for graphlet in DATA_MOTIFS[ego]:\n",
    "            s[graphlet] += int(DATA_MOTIFS[ego][graphlet])\n",
    "    \n",
    "    all_graphlets = float(sum(s.values()))\n",
    "    local_representativities = {x : s[x]/all_graphlets for x in s}\n",
    "    \n",
    "    return {x : local_representativities[x] / float(OVERALL_REPR[x]) \\\n",
    "            if local_representativities[x] / float(OVERALL_REPR[x]) < 1\n",
    "            else\n",
    "                2 - 1 / (local_representativities[x] / float(OVERALL_REPR[x]))\n",
    "            for x in local_representativities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6_4\n",
      "6_2\n",
      "5_1\n",
      "5_2\n",
      "4_3\n",
      "4_1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "corpus = 'facebook'\n",
    "k = 5\n",
    "\n",
    "FILE_NAME = '%s_k%s' % (corpus, k)\n",
    "MAIN_DIR = '../Results/%s' % FILE_NAME\n",
    "\n",
    "create_sankey()\n",
    "create_simple_sankey()\n",
    "\n",
    "clusters = []\n",
    "\n",
    "#On garde les derniers clusters de chaque chaine\n",
    "with open('%s/simple_sankey.csv' % MAIN_DIR, 'r') as to_read:\n",
    "    csv_r = csv.reader(to_read, delimiter = ',')\n",
    "    csv_r.next()\n",
    "    for line in csv_r:\n",
    "        if not line[1] in clusters:\n",
    "            clusters.append(line[1])\n",
    "        if line[0] in clusters:\n",
    "            clusters.remove(line[0])\n",
    "            \n",
    "clusters.sort(reverse = True, key = lambda x : x.split('_')[0])\n",
    "egos, DATA_MOTIFS, keys = get_data('../Data/motifs_%s.csv' % FILE_NAME)\n",
    "DATA_MOTIFS = get_data('../Data/motifs_%s.csv' % FILE_NAME, representativity = False)[1]\n",
    "OVERALL_REPR = get_overall_representativity()\n",
    "\n",
    "egos_ok = []\n",
    "clusters_values, clusters_size = {}, {}\n",
    "\n",
    "for cluster in clusters:\n",
    "    size, classe = [int(x) for x in cluster.split('_')]\n",
    "    \n",
    "    with open('%s/Typo_%s/egos_classe_%s.csv' % (MAIN_DIR, size, classe)) as to_read:\n",
    "        csv_r = csv.reader(to_read, delimiter = ';')\n",
    "        csv_r.next()\n",
    "        list_egos = []\n",
    "        for line in csv_r:\n",
    "            ego = line[0]\n",
    "            if not ego in egos_ok:\n",
    "                list_egos.append(ego)\n",
    "                egos_ok.append(ego)\n",
    "        print cluster\n",
    "        clusters_values[cluster] = get_classe_representativity(list_egos)\n",
    "        clusters_size[cluster] = len(list_egos)\n",
    "        \n",
    "with open('%s/Typo_%s/kmeans_stats.csv' % (MAIN_DIR, 'sankey'), 'w') as to_write:\n",
    "    csv_w = csv.writer(to_write, delimiter = ';')\n",
    "    csv_w.writerow(['classe', 'nb',''] + keys)\n",
    "    for cluster in clusters_values:\n",
    "        csv_w.writerow([cluster, clusters_size[cluster],''] + [clusters_values[cluster][k] for k in keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
